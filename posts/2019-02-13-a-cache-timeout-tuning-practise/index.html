

<!DOCTYPE html>
<html lang="en">
    <head>
      <meta http-equiv="X-UA-Compatible" content="IE=edge">
      <meta http-equiv="content-type" content="text/html; charset=utf-8">

      <!-- Enable responsiveness on mobile devices-->
      <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

      <title>wangbo’s home &amp; blogs</title>

      

      
          <script src="https://cdnjs.cloudflare.com/ajax/libs/slideout/1.0.1/slideout.min.js"></script>
          
      

      
          <link rel="stylesheet" href="https:&#x2F;&#x2F;www.wangbo.im&#x2F;site.css">
          
      

      
      
    </head>

    <body>
        <div class="container">

            <div id="mobile-navbar" class="mobile-navbar">
              <div class="mobile-header-logo">
                <a href="/" class="logo">WB Prime</a>
              </div>
              <div class="mobile-navbar-icon icon-out">
                <span></span>
                <span></span>
                <span></span>
              </div>
            </div>

            <nav id="mobile-menu" class="mobile-menu slideout-menu slideout-menu-left">
              <ul class="mobile-menu-list">
                
                    <li class="mobile-menu-item">
                        <a href="https://www.wangbo.im">
                            Home
                        </a>
                    </li>
                
                    <li class="mobile-menu-item">
                        <a href="https://www.wangbo.im&#x2F;categories">
                            Categories
                        </a>
                    </li>
                
                    <li class="mobile-menu-item">
                        <a href="https://www.wangbo.im&#x2F;tags">
                            Tags
                        </a>
                    </li>
                
                    <li class="mobile-menu-item">
                        <a href="https://www.wangbo.im&#x2F;about">
                            About
                        </a>
                    </li>
                
              </ul>
            </nav>

            <header id="header">
                <div class="logo"><a href="https:&#x2F;&#x2F;www.wangbo.im">WB Prime</a></div>
                <nav class="menu">
                    <ul>
                        
                            <li>
                                <a href="https://www.wangbo.im">
                                    Home
                                </a>
                            </li>
                        
                            <li>
                                <a href="https://www.wangbo.im&#x2F;categories">
                                    Categories
                                </a>
                            </li>
                        
                            <li>
                                <a href="https://www.wangbo.im&#x2F;tags">
                                    Tags
                                </a>
                            </li>
                        
                            <li>
                                <a href="https://www.wangbo.im&#x2F;about">
                                    About
                                </a>
                            </li>
                        
                    </ul>
                </nav>
            </header>

            <main>
                <div class="content" id="mobile-panel">
                    


<div class="post-toc" id="post-toc">
    <h2 class="post-toc-title">Contents</h2>
    <div class="post-toc-content always-active">
        <nav id="TableOfContents">
            <ul>
                
                <li>
                    <a href="https://www.wangbo.im/posts/2019-02-13-a-cache-timeout-tuning-practise/#bei-jing" class="toc-link">背景</a>
                    
                </li>
                
                <li>
                    <a href="https://www.wangbo.im/posts/2019-02-13-a-cache-timeout-tuning-practise/#pai-cha" class="toc-link">排查</a>
                    
                    <ul>
                        
                        <li>
                            <a href="https://www.wangbo.im/posts/2019-02-13-a-cache-timeout-tuning-practise/#cache-ke-hu-duan" class="toc-link">Cache 客户端</a>
                        </li>
                        
                        <li>
                            <a href="https://www.wangbo.im/posts/2019-02-13-a-cache-timeout-tuning-practise/#bu-shu-ji-qi" class="toc-link">部署机器</a>
                        </li>
                        
                        <li>
                            <a href="https://www.wangbo.im/posts/2019-02-13-a-cache-timeout-tuning-practise/#wang-luo-zhua-bao" class="toc-link">网络抓包</a>
                        </li>
                        
                        <li>
                            <a href="https://www.wangbo.im/posts/2019-02-13-a-cache-timeout-tuning-practise/#tcp-kuai-su-zhong-chuan" class="toc-link">TCP 快速重传</a>
                        </li>
                        
                        <li>
                            <a href="https://www.wangbo.im/posts/2019-02-13-a-cache-timeout-tuning-practise/#cache-fu-wu-qi-zhua-bao" class="toc-link">Cache 服务器抓包</a>
                        </li>
                        
                        <li>
                            <a href="https://www.wangbo.im/posts/2019-02-13-a-cache-timeout-tuning-practise/#sheng-ji-si-you-yun-shi-li-pei-zhi" class="toc-link">升级私有云实例配置</a>
                        </li>
                        
                    </ul>
                    
                </li>
                
                <li>
                    <a href="https://www.wangbo.im/posts/2019-02-13-a-cache-timeout-tuning-practise/#hou-xu" class="toc-link">后续</a>
                    
                    <ul>
                        
                        <li>
                            <a href="https://www.wangbo.im/posts/2019-02-13-a-cache-timeout-tuning-practise/#client-server-router-quan-lu-jing-zhua-bao" class="toc-link">Client &amp; Server &amp; Router 全路径抓包</a>
                        </li>
                        
                        <li>
                            <a href="https://www.wangbo.im/posts/2019-02-13-a-cache-timeout-tuning-practise/#dui-bi-fu-wu-shi-li" class="toc-link">对比服务实例</a>
                        </li>
                        
                        <li>
                            <a href="https://www.wangbo.im/posts/2019-02-13-a-cache-timeout-tuning-practise/#sheng-ji-cache-fu-wu-qi-dao-mo-zhao-wang-qia" class="toc-link">升级 Cache 服务器到万兆网卡</a>
                        </li>
                        
                    </ul>
                    
                </li>
                
                <li>
                    <a href="https://www.wangbo.im/posts/2019-02-13-a-cache-timeout-tuning-practise/#zong-jie" class="toc-link">总结</a>
                    
                </li>
                
            </ul>
        </nav>
    </div>
</div>


<article class="post">
    
    <header class="post__header">
        <h1 class="post__title">
            <a href="https:&#x2F;&#x2F;www.wangbo.im&#x2F;posts&#x2F;2019-02-13-a-cache-timeout-tuning-practise&#x2F;">一次 Cache 问题排查实践</a>
        </h1>
        <div class="post__meta">
            <span class="post__time">2019-02-13</span>
            
        </div>
    </header>

    <div class="post-content">
      <p>负责的服务在线上环境出现了丢弃请求的现象。经排查是由于请求队列中的若干请求处理时间过长，导致队列之后的请求等待以致超时，而在队列中超时的请求会被框架直接抛弃掉。</p>
<p>排查服务日志，发现访问 Cache 超时的量有明显增加；较多的 Cache 超时导致更多的不必要 DB 访问，以致请求的响应时间变长；同时，更多的 DB 访问压力也会对 DB 的平均响应时间有影响；Cache 访问和 DB 访问超时导致请求响应超时，从而导致抛弃量。</p>
<p>问题在于 Cache 访问为什么会出现大量超时？一路排查下去发现有可能是 TCP 丢包导致的快速重传(fast retransmission)引起的。</p>
<p><a name="continue-reading"></a></p>
<h1 id="bei-jing"><a class="zola-anchor" href="#bei-jing" aria-label="Anchor link for: bei-jing">🔗</a>
背景</h1>
<p>服务的正常情况下的峰值请求访问量 ~ 200w，此状态下，服务的各项指标正常，Cache 访问超时量为个位数；春节期间，由于流量激增至峰值达370w，开始出现请求访问超时以及请求被抛弃现象。</p>
<p>排查服务日志，发现服务访问 Cache 超时的量显著增加，达到万的数量级。由于 Cache 超时，会导致多一次不必要的 DB 访问，使得访问时间显著增加；同时由于 DB 访问量上升，使得 DB 的平均访问时间也增加，导致请求响应时间进一步恶化。</p>
<p>服务一共部署了 5 个以上的实例；访问的 Cache 为分布式 memcache 服务器集群，实例个数为 8 个。</p>
<h1 id="pai-cha"><a class="zola-anchor" href="#pai-cha" aria-label="Anchor link for: pai-cha">🔗</a>
排查</h1>
<h2 id="cache-ke-hu-duan"><a class="zola-anchor" href="#cache-ke-hu-duan" aria-label="Anchor link for: cache-ke-hu-duan">🔗</a>
Cache 客户端</h2>
<p>服务所使用的 Cache 底层是 memcache，通过一致性哈希形成了分布式的缓存集群。Cache 请求的 dispatch 通过客户端在调用方服务中进行。</p>
<ol>
<li>servers</li>
</ol>
<p>由于是分布式缓存，对于一个操作（如 GET），会首先根据哈希选出目标 Server。Server 表示一个固定的memcache 服务器。每一个 Server 在初始化的时候会根据配置参数新建若干 Socket，用来与固定的 memcache 服务端进行交互。</p>
<p>对 Scoket 的读写是通过 NIO Selector 进行的。</p>
<p><img src="https://www.wangbo.im/posts/2019-02-13-a-cache-timeout-tuning-practise/cache_client.loop.png" alt="cache client overview" /></p>
<ol start="2">
<li>requests</li>
</ol>
<p>请求是同步处理的。</p>
<p>请求线程需要首先序列化请求为字节流，然后找到对应的 Server，将请求放到一个全局的阻塞队列中。</p>
<p>全局阻塞队列中的请求会被异步发送之选中的 memcache 服务器；请求线程会根据超时时间来阻塞地等待服务端的响应，由 <code>CountdownLatch</code> 实现。</p>
<ol start="3">
<li>write worker</li>
</ol>
<p>一个单独的 write 线程用来将请求发送给选中的服务端。</p>
<p>write 线程轮循请求的全局阻塞队列，将序列化好的请求通过在 Server 所维护的任一个 Socket 发送至服务端。</p>
<p>此过程是顺序、单线程的。</p>
<pre style="background-color:#2b303b;">
<span style="color:#c0c5ce;">loop {
</span><span style="color:#c0c5ce;">    </span><span style="color:#c0c5ce;">session = sessionsQueue.poll(1000L, TimeUnit.MILLISECONDS) // blocking here
</span><span style="color:#c0c5ce;">    </span><span style="color:#c0c5ce;">lock(socket) {
</span><span style="color:#c0c5ce;">        </span><span style="color:#c0c5ce;">buf = session.bytes()
</span><span style="color:#c0c5ce;">        </span><span style="color:#c0c5ce;">while (buf.hasRemaining()) {
</span><span style="color:#c0c5ce;">            </span><span style="color:#c0c5ce;">socket.write(buf) // non-blocking
</span><span style="color:#c0c5ce;">        </span><span style="color:#c0c5ce;">}
</span><span style="color:#c0c5ce;">    </span><span style="color:#c0c5ce;">}
</span><span style="color:#c0c5ce;">}
</span></pre>
<blockquote>
<p>由于 write 操作是非阻塞的，单个会话字节流的发送需要放在一个循环中，这个过程可以优化为处理 Selector
的 writable 消息。</p>
</blockquote>
<ol start="4">
<li>read worker</li>
</ol>
<p>一个单独的 read 线程用来处理服务端返回的响应。</p>
<p>read 线程阻塞于 NIO Selector 之 select 方法。一旦有读事件就绪，就会处理该事件：读取字节流并反序列化为 sessionId，通过该 sessionId 找到对应的会话并唤醒该会话对应的请求线程（通过 <code>CountdownLatch</code>）。</p>
<p>此过程是顺序、单线程的。</p>
<pre style="background-color:#2b303b;">
<span style="color:#c0c5ce;">loop {
</span><span style="color:#c0c5ce;">    </span><span style="color:#c0c5ce;">Selector.select(); // blocking here
</span><span style="color:#c0c5ce;">    </span><span style="color:#c0c5ce;">foreach selectionKey in Selector.selectedKeys() {
</span><span style="color:#c0c5ce;">        </span><span style="color:#c0c5ce;">if (selectionKey.readyOps() &amp; READ) {
</span><span style="color:#c0c5ce;">            </span><span style="color:#c0c5ce;">selectionKey.channel().read(buf)
</span><span style="color:#c0c5ce;">            </span><span style="color:#c0c5ce;">sessions = Decoder.decode(buf)
</span><span style="color:#c0c5ce;">
</span><span style="color:#c0c5ce;">            </span><span style="color:#c0c5ce;">foreach session in sessions {
</span><span style="color:#c0c5ce;">                </span><span style="color:#c0c5ce;">notify(session) // non-blocking
</span><span style="color:#c0c5ce;">            </span><span style="color:#c0c5ce;">}
</span><span style="color:#c0c5ce;">        </span><span style="color:#c0c5ce;">}
</span><span style="color:#c0c5ce;">    </span><span style="color:#c0c5ce;">}
</span><span style="color:#c0c5ce;">}
</span></pre>
<blockquote>
<p>感觉本过程可以优化为：每一个 Socket 对应一个 read worker，而不是现在的所有的 Socket 对应同一个
read worker。目前的逻辑，在 Server 的负载非常大的时候可能会在处理单个 Server 的响应上花费较多时间
导致其他的 Server 的响应处理不及时。</p>
</blockquote>
<ol start="5">
<li>responses</li>
</ol>
<p>请求线程被唤醒之后，会判断唤醒原因是成功收取结果还是超时。如果成功收取到 memcache 服务器的返回，则会进行反序列化工作，并使用反序列化的结果进行后续的处理。</p>
<h2 id="bu-shu-ji-qi"><a class="zola-anchor" href="#bu-shu-ji-qi" aria-label="Anchor link for: bu-shu-ji-qi">🔗</a>
部署机器</h2>
<p>服务部署与公司内部的私有云（docker based）上，设置的单实例限制为 CPU 8 核；实例所在的物理机为 40 核。</p>
<p>在服务实例内部通过 <code>top</code> 查看服务器的 CPU 使用情况如下（这个应该是物理机的数据）：</p>
<pre style="background-color:#2b303b;">
<span style="color:#c0c5ce;">load average: 15.51, 16.54, 16.28
</span><span style="color:#c0c5ce;">Cpu(s): 29.4%us,  6.0%sy,  0.0%ni, 60.3%id,  0.1%wa,  0.0%hi,  4.3%si,  0.0%st
</span></pre>
<p>服务进程的 CPU 使用率为 273%；read worker 和 write worker 线程的 CPU 占用率分别为 14.6% 和 21.9%。从数据上看 Cache 客户端读写的压力不大，应该不会存在由于负载过高导致的请求延时。</p>
<p>私有云实例分配的网卡限制为入流量最大 40M/s，初流量最大 40M/s，通过监控可以看到每分钟流量平均值的最大值：入流量为 9.1 M/s，出流量为 5.76 M/s。流量上也没有瓶颈。</p>
<h2 id="wang-luo-zhua-bao"><a class="zola-anchor" href="#wang-luo-zhua-bao" aria-label="Anchor link for: wang-luo-zhua-bao">🔗</a>
网络抓包</h2>
<p>从应用层和服务器硬件层暂时看不出来问题，考虑进行网络抓包分析。</p>
<p>使用 <code>tcpdump</code> 进行抓包，同时监控服务日志，确保能抓取到 Cache 超时的网络包数据。</p>
<pre style="background-color:#2b303b;">
<span style="color:#c0c5ce;">sudo tcpdump -i any host x.x.x.x -w cache.$(date +%Y%m%dT%H%M%S).pcap
</span></pre>
<p>抓取到的网络包文件下载到本地，使用 <code>wireshark</code> 进行分析。由于 Cache 底层使用的 memcache 的协议，可以直接使用 <code>wireshark</code> 分析 memcache 的报文，因为 <code>wireshark</code> 内建了对 memcache 的支持。</p>
<p>对抓取到的网络包进行过滤，使用过滤器 “memcache.opaque”：memcache 的请求报文里面有一个特殊字段 opaque，可以用来进行会话跟踪；如果某个请求中包含了 opaque 值，则对应的响应中也会包含相同的 opaque 值。对于超时的 Cache 请求，服务日志中会打印出相应的 opaque 值。这样，在服务日志中找到超时请求的 opaque 值，在 <code>wireshark</code> 中找到对应的包，可以很方便地进行分析。</p>
<p><img src="https://www.wangbo.im/posts/2019-02-13-a-cache-timeout-tuning-practise/wireshark.filter_by_opaque.timeout.png" alt="wireshark graph by opaque filter for timeout request" /></p>
<p>case 1: 服务日志中显示超时，抓包也显示过期(&gt; 50 ms)的会话</p>
<p><img src="https://www.wangbo.im/posts/2019-02-13-a-cache-timeout-tuning-practise/wireshark.filter_by_opaque.normal.png" alt="wireshark graph by opaque filter for normal request" /></p>
<p>case 2: 服务日志中显示超时，抓包显示不过期(&lt; 50 ms)的会话</p>
<p>实际排查发现，服务日志中一旦出现请求超时就会是连续的几十个会话都超时（本次是xx个）。但是在<code>wireshark</code> 中可以看到，只有最开始的 6 个会话是网络层超时的；后续的会话都没有超时，并且请求包发出的时间和服务日志中显示的 Socket 写入的时间之间间隔很小（微秒级），同时可以看到一些 fast retransmission和 Out-Of-Order 的包。</p>
<p>也就是 6 个在网络层超时的会话，以及后面的连续多个在网络层没有超时的会话，在应用层都表现为超时。</p>
<p><img src="https://www.wangbo.im/posts/2019-02-13-a-cache-timeout-tuning-practise/wireshark.fast_retransmission.png" alt="wireshark graph for fast retransmission" /></p>
<h2 id="tcp-kuai-su-zhong-chuan"><a class="zola-anchor" href="#tcp-kuai-su-zhong-chuan" aria-label="Anchor link for: tcp-kuai-su-zhong-chuan">🔗</a>
TCP 快速重传</h2>
<p>回忆一下 TCP 的拥塞控制，当发送方收到 3 次对同一个包的重复 Ack 之后，会推断该包在传输过程中已丢失，从而启动快速重传过程。快速重传过程中滑动窗口停止滑动，等待接收方对重传包的确认；可以看到Out-Of-Order Ack 和 fast retransmission 之前有几个长度较大的请求包，推测是由于这些包的导致了 duplicate Ack。</p>
<p>快速重传会导致响应包与请求包之间的时间比正常大；但是对于抓包显示不超时的请求为什么在应用层显示为超时，还是无法解释。</p>
<p>一个可能的解释是：发送端在快速重传过程中会暂停后续数据包的发送，使得数据包在缓冲区等待较长的时间，以致请求在应用层超时。</p>
<p>网络层的事情已经招运维进一步排查。</p>
<h2 id="cache-fu-wu-qi-zhua-bao"><a class="zola-anchor" href="#cache-fu-wu-qi-zhua-bao" aria-label="Anchor link for: cache-fu-wu-qi-zhua-bao">🔗</a>
Cache 服务器抓包</h2>
<p>在 Cache 服务器抓包，显示服务端处理时间正常。</p>
<h2 id="sheng-ji-si-you-yun-shi-li-pei-zhi"><a class="zola-anchor" href="#sheng-ji-si-you-yun-shi-li-pei-zhi" aria-label="Anchor link for: sheng-ji-si-you-yun-shi-li-pei-zhi">🔗</a>
升级私有云实例配置</h2>
<p>由于故障发生在春节流量高峰，怀疑问题是由于流量高峰引起的。尝试升级私有云服务实例的 CPU 核数到 10 核，并扩容。</p>
<p>升级后的服务实例貌似 Cache 超时量减少。</p>
<h1 id="hou-xu"><a class="zola-anchor" href="#hou-xu" aria-label="Anchor link for: hou-xu">🔗</a>
后续</h1>
<h2 id="client-server-router-quan-lu-jing-zhua-bao"><a class="zola-anchor" href="#client-server-router-quan-lu-jing-zhua-bao" aria-label="Anchor link for: client-server-router-quan-lu-jing-zhua-bao">🔗</a>
Client & Server & Router 全路径抓包</h2>
<p>跟腾讯机房的负责人一起抓包，先是各种抓包文件太大导出失败，后来是超时现象不复现。</p>
<p>通过在服务端（Cache 之 client 端）分析，发现只有 5 个服务实例出现 Cache 超时了。一直对 Cache 服务器
超时的服务实例的超时现象不见了。灵异事件！！！</p>
<h2 id="dui-bi-fu-wu-shi-li"><a class="zola-anchor" href="#dui-bi-fu-wu-shi-li" aria-label="Anchor link for: dui-bi-fu-wu-shi-li">🔗</a>
对比服务实例</h2>
<p>分别对比出现超时的服务实例和不出现超时的服务实例，包括 <code>top</code>（CPU 使用率、负载、Java 进程）、<code>free</code>
（内存使用）、<code>mpstat</code>（CPU 核心负载均衡）、<code>vmstat</code>（CPU 执行队列）、<code>iostat</code>（文件系统 IO 压力）、
<code>sar</code>（网络 IO 压力）、<code>numastat</code>（NUMA 配置）等，发现基本上两类实例没有显著区别，在个别参数上甚至不
超时的服务实例的压力更大。</p>
<h2 id="sheng-ji-cache-fu-wu-qi-dao-mo-zhao-wang-qia"><a class="zola-anchor" href="#sheng-ji-cache-fu-wu-qi-dao-mo-zhao-wang-qia" aria-label="Anchor link for: sheng-ji-cache-fu-wu-qi-dao-mo-zhao-wang-qia">🔗</a>
升级 Cache 服务器到万兆网卡</h2>
<p>升级所有的 Cache 服务器，将千兆网卡升级为万兆网卡。实测没有解决本质问题。</p>
<h1 id="zong-jie"><a class="zola-anchor" href="#zong-jie" aria-label="Anchor link for: zong-jie">🔗</a>
总结</h1>
<p>总而言之，这一次的问题排查属于无疾而终。</p>
<p>大概的原因，推测是中间交换机导致的 TCP 超时引发的包重传，加上 Cache 客户端的单线程读写（BUGLY）的处理逻辑
，加上大流量导致的高并发。由于服务部署在私有云上，很多可用的分析工具无法使用，导致不能在问题发生期间
快速定位问题，此需引以为戒。</p>
<p>后续继续跟进。</p>

    </div>

    
    

    <div class="post-footer">
        
            
                <div class="post-tags">
                    
                        <a href="https:&#x2F;&#x2F;www.wangbo.im&#x2F;tags&#x2F;java&#x2F;">#java</a>
                    
                        <a href="https:&#x2F;&#x2F;www.wangbo.im&#x2F;tags&#x2F;cache&#x2F;">#cache</a>
                    
                        <a href="https:&#x2F;&#x2F;www.wangbo.im&#x2F;tags&#x2F;tcp&#x2F;">#tcp</a>
                    
                        <a href="https:&#x2F;&#x2F;www.wangbo.im&#x2F;tags&#x2F;fast-retransmit&#x2F;">#fast retransmit</a>
                    
                        <a href="https:&#x2F;&#x2F;www.wangbo.im&#x2F;tags&#x2F;congistion&#x2F;">#congistion</a>
                    
                </div>
            
            
                <div class="post-nav">
                    
                        <a class="previous" href="https:&#x2F;&#x2F;www.wangbo.im&#x2F;posts&#x2F;2019-02-01-a-gc-tuning-practise-involving-mysql&#x2F;">‹ JAVA 服务从物理机迁移到 Docker 私有云背景下的 GC 调优实践</a>
                    
                    
                        <a class="next" href="https:&#x2F;&#x2F;www.wangbo.im&#x2F;posts&#x2F;2019-03-11-a-online-issue-caused-by-dns-unavailability&#x2F;">一次由 DNS 解析服务失效引发的线上 DB 连接打满的事故经历 ›</a>
                    
                    
                    
                </div>
            

        

    </div>

    
    
</article>


                </div>
            </main>

            
            
        </div>

      
          <script type="text/javascript" src="https:&#x2F;&#x2F;www.wangbo.im&#x2F;even.js" ></script>
      
    </body>

</html>
